# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "b112109a-51a8-4b2e-bfa2-df2d8f6fa1f3",
# META       "default_lakehouse_name": "lh_sales_analytics",
# META       "default_lakehouse_workspace_id": "b2bbdee6-3aaf-4818-9a42-64f24bc5c285",
# META       "known_lakehouses": [
# META         {
# META           "id": "b112109a-51a8-4b2e-bfa2-df2d8f6fa1f3"
# META         }
# META       ]
# META     },
# META     "environment": {
# META       "environmentId": "df8fac0b-9074-99ff-45d3-3861991da1b5",
# META       "workspaceId": "00000000-0000-0000-0000-000000000000"
# META     }
# META   }
# META }

# MARKDOWN ********************

# # ðŸ¥‰ Bronze Layer â€” Data Ingestion
# 
# **Objective**
# 
# This notebook is responsible for ingesting raw purchase data into the Bronze layer,
# preserving the source structure while adding technical ingestion metadata.
# 
# **Execution Context**
# - **Platform:** Microsoft Fabric
# - **Compute:** Spark (Lakehouse attached)
# - **Storage format:** Delta Lake
# - **Execution mode:** Batch
# - **Orchestration:** Fabric Data Pipeline

# MARKDOWN ********************

# ## Runtime Configuration
# 
# This section defines runtime dependencies, execution parameters,
# and pipeline context required for the ingestion process.

# CELL ********************

# ============================================================
# Runtime Dependencies
# ============================================================

from pyspark.sql import functions as F
from pyspark.sql.types import *

# ============================================================
# Execution Context
# ============================================================

ENV = "dev"
LAYER = "bronze"
PIPELINE_NAME = "bronze_ingestion"

# ============================================================
# Orchestration Context
# ============================================================

batch_id = spark.conf.get("pipeline.batchId", "manual_run")

# ============================================================
# Source Configuration
# ============================================================

SOURCE_PATH = "Files/purchases.json"

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Raw Data Ingestion
# 
# This section loads raw data from the source system
# without applying business transformations.

# CELL ********************

# ============================================
# Raw Data Ingestion
# ============================================

# Read multi-line JSON file generated by the synthetic dataset generator
df_raw = (
    spark.read
         .option("multiLine", True)  # Required for JSON files with nested structures
         .json(SOURCE_PATH)
)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Minimal Structural Shaping
# 
# This section applies minimal structural adjustments required
# to make the dataset queryable while preserving source semantics.

# CELL ********************

# ============================================
# Structural Normalization
# ============================================

# Explode the 'data' array so that each purchase becomes one row
df_purchases = (
    df_raw
        .select(F.explode(F.col("data")).alias("purchase"))
        .select("purchase.*")
)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Ingestion Metadata
# 
# This section enriches the dataset with technical metadata
# to support lineage, auditing, and reprocessing.

# CELL ********************

# ============================================
# Ingestion Metadata
# ============================================

df_purchases = (
    df_purchases
        # Timestamp indicating when the record was ingested
        .withColumn("ingestion_timestamp", F.current_timestamp())
        
        # Logical source identifier
        .withColumn("source_system", F.lit(SOURCE_PATH))
        
        # Batch identifier propagated from the pipeline
        .withColumn("batch_id", F.lit(batch_id))
)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Load Validation & Metrics
# 
# This section provides basic observability for the ingestion process,
# ensuring data availability and operational transparency.

# CELL ********************

# ============================================
# Load Validation & Metrics
# ============================================

# Ensure that at least one record was ingested
assert df_purchases.count() > 0, "Purchases dataset is empty"

print(f"Purchases loaded: {df_purchases.count()}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Persisting Data to Bronze
# 
# Data is stored in Delta format using overwrite mode,
# ensuring idempotent and reprocessable ingestion.

# CELL ********************

# ============================================
# Persisting Data to Bronze
# ============================================

df_purchases.write \
    .format("delta") \
    .mode("overwrite") \
    .saveAsTable("bronze.purchases")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
